{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Multitask Learning for Geometric Shape Classification and Counting**\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "In this project, you will design, implement, and evaluate a **multitask neural network** that performs **two tasks simultaneously**:\n",
        "\n",
        "1. **Classification** – identify which pair of geometric shape types appears in a 28×28 binary image (135 possible configurations).\n",
        "2. **Regression** – predict how many shapes of each type are present (6 regression targets).\n",
        "\n",
        "This project focuses on **multi-task learning**, i.e., using one shared model to learn several related tasks at once. You will compare how adding an auxiliary task affects performance and training dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "You will use the **Geometric Shape Numbers (GSN)** dataset:\n",
        "\n",
        "```bash\n",
        "!wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "```\n",
        "\n",
        "This will create a directory `data/` containing:\n",
        "\n",
        "* **10,000 images** (28×28x1, grayscale)\n",
        "* **labels.csv** – counts of each of six shape types per image\n",
        "\n",
        "Each image contains exactly **two types** of geometric figures (out of six) and **10 shapes total**.\n",
        "\n",
        "**Shape classes:**\n",
        "\n",
        "| Index | Shape type     |\n",
        "| ----: | -------------- |\n",
        "|     0 | square         |\n",
        "|     1 | circle         |\n",
        "|     2 | triangle up    |\n",
        "|     3 | triangle right |\n",
        "|     4 | triangle down  |\n",
        "|     5 | triangle left  |\n",
        "\n",
        "Example row from `labels.csv`:\n",
        "\n",
        "```\n",
        "name,squares,circles,up,right,down,left\n",
        "img_00000.png,0,0,0,4,0,6\n",
        "```\n",
        "\n",
        "Here, the image contains **4 right-pointing triangles** and **6 left-pointing triangles**.\n",
        "\n",
        "**Split:**\n",
        "\n",
        "* Training: first 9,000 samples\n",
        "* Validation: last 1,000 samples\n",
        "\n",
        "Examples:\n",
        "![example.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Tasks and objectives\n",
        "\n",
        "You must design a **multitask deep learning system** that:\n",
        "\n",
        "1. **Classifies** each image into one of **135 possible configurations**, representing:\n",
        "\n",
        "   * which **two shape classes** appear, and\n",
        "   * how their counts (1–9) sum to 10.\n",
        "\n",
        "   → Example: \"3 circles + 7 squares\" is one configuration class.\n",
        "\n",
        "2. **Regresses** the number of shapes of each type (a 6-dimensional real-valued output).\n",
        "\n",
        "3. Combines both objectives in a **joint loss** function (Hint: losses are implemented in PyTorch):\n",
        "\n",
        "\n",
        "$$ Loss = \\text{NLLLoss(classification)} + \\lambda_{\\text{cnt}} \\cdot \\text{SmoothL1Loss(regression)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model requirements\n",
        "\n",
        "### Architecture constraints\n",
        "\n",
        "You must use **exactly this feature extractor (backbone)**:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        ")\n",
        "```\n",
        "\n",
        "Then add **two separate heads**:\n",
        "\n",
        "* `head_cls`: outputs log-probabilities for 135 classes\n",
        "* `head_cnt`: outputs 6 regression values (counts)\n",
        "\n",
        "The model must return two outputs: `(log_probs, counts)`.\n",
        "\n",
        "You may add dropout or batch normalization inside the heads, **but you must not modify the backbone**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Training setup\n",
        "\n",
        "* Optimizer: **Adam**, learning rate = 1e-3\n",
        "* Epochs: up to **100** (use **early stopping**)\n",
        "* Batch sizes: **64** (train), **1000** (validation)\n",
        "* Device: GPU allowed for Notebook, but your **final code must run on GPU within ~30 minutes**\n",
        "* Random seed: set `torch.manual_seed(1)` for reproducibility\n",
        "* Split: **exactly 9,000 train / 1,000 validation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data preprocessing and augmentation\n",
        "\n",
        "You must implement a **PyTorch `Dataset` class** that:\n",
        "\n",
        "* Reads `labels.csv`\n",
        "* Loads the corresponding image (from `data/`)\n",
        "* Returns both:\n",
        "  * the image (as a tensor)\n",
        "  * the labels (counts for 6 shapes)\n",
        "* Optionally applies transformations\n",
        "\n",
        "### Required augmentations\n",
        "\n",
        "You must implement **at least three** of the following:\n",
        "\n",
        "1. Random horizontal flip\n",
        "2. Random vertical flip\n",
        "3. Random 90° rotation (must correctly rotate orientation labels: up → right → down → left)\n",
        "4. Random brightness/contrast (mild)\n",
        "5. Gaussian noise\n",
        "6. Random erasing (small areas only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation metrics\n",
        "\n",
        "Implement and report the following metrics on the validation set:\n",
        "\n",
        "### (a) **Classification (135-way)**\n",
        "\n",
        "* Top-1 accuracy\n",
        "* Macro F1-score\n",
        "* Per-pair accuracy (aggregate by unordered shape pair, e.g. {circle, up})\n",
        "\n",
        "### (b) **Regression (6-D counts)**\n",
        "\n",
        "* RMSE per class and overall\n",
        "* MAE per class and overall\n",
        "\n",
        "Also plot:\n",
        "\n",
        "* Training and validation losses\n",
        "* Validation accuracy and RMSE over epochs\n",
        "\n",
        "**Important**: This task is not about finding the best architecture; we expect at least 50% accuracy, but achieving results higher than that will not affect the grade for the assignment**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Experiments and analysis\n",
        "\n",
        "You must train and compare **three model settings**:\n",
        "\n",
        "| Setting | Description                                      |\n",
        "| :------ | :----------------------------------------------- |\n",
        "| 1       | **Classification-only:** λ_cnt = 0               |\n",
        "| 2       | **Regression-only:** classification loss ignored |\n",
        "| 3       | **Multitask:** λ_cnt = with your choose          |\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "* Train until early stopping\n",
        "* Record loss, accuracy, RMSE, and runtime\n",
        "* Compare results and explain how λ influences learning\n",
        "* Discuss whether multitask learning improves the main tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Final deliverables\n",
        "\n",
        "You must submit .zip project with:\n",
        "\n",
        "1. **Code** (`.ipynb` or `.py`) that:\n",
        "\n",
        "   * Downloads and extracts the dataset\n",
        "   * Defines dataset, dataloaders, model, loss, training loop, evaluation, and plotting\n",
        "   * Can run start-to-end without interaction, and finishes within 30 minutes on Colab T4 GPUs\n",
        "   * Includes three experiment configurations\n",
        "\n",
        "2. **Report (2–4 pages, PDF)** including:\n",
        "   * Section on (EDA) Exploratory Data Analysis in your report: no more than 3 graphs or tables describing the data set.\n",
        "   * Model architecture\n",
        "   * Description and justification of augmentations\n",
        "   * Results table (loss, accuracy, RMSE for all runs)\n",
        "   * Learning curves\n",
        "   * Discussion on multitask effects\n",
        "\n",
        "3. **README.md**:\n",
        "\n",
        "   * Link to Colab version of task for fast replication.\n",
        "   * Approximate runtime and resource requirements\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Grading rubric\n",
        "\n",
        "Component\tDescription\tPoints\n",
        "1. Implementation correctness\tCorrect use of the fixed backbone, two-headed model, and proper training loop (classification + regression).\t30%\n",
        "2. Data & augmentations\tProper dataset loading, preprocessing, and at least three augmentations with brief justification.\t20%\n",
        "3. Evaluation & experiments\tCorrect computation of metrics (accuracy, F1, RMSE) and completion of all three λ configurations (λ=0, regression-only, your choice λ).\t30%\n",
        "4. Report & analysis\n",
        "A clear separation of concerns (e.g. headers in notebooks, modules in code) and concise 2–4 page report with results tables, learning curves, confusion matrix, and short discussion on multitask effects and error examples.\n",
        "20%\n",
        "\n",
        "###### Readability and modularity will be considered within each grading component. Clear structure (headers in notebooks, docstrings, modular code) significantly improves evaluation speed. Emphasize using clear headers to help reviewers navigate efficiently.\n",
        "---"
      ],
      "metadata": {
        "id": "_NvRrg8YvTPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmggC-uvcGR",
        "outputId": "063b5d46-86b3-4426-f334-8a6288f88fe7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-24 11:18:42--  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip [following]\n",
            "--2025-11-24 11:18:42--  https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5544261 (5.3M) [application/zip]\n",
            "Saving to: ‘data_gsn.zip’\n",
            "\n",
            "data_gsn.zip        100%[===================>]   5.29M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-11-24 11:18:43 (132 MB/s) - ‘data_gsn.zip’ saved [5544261/5544261]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=1):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqysQdcthhTy",
        "outputId": "a299d3a1-d45c-4cfd-a374-f20ee113ad51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GSNDataset(Dataset):\n",
        "    def __init__(self, root_dir, csv_file, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with all the images.\n",
        "            csv_file (str): Path to the csv file with annotations.\n",
        "            split (str): 'train' (first 9000) or 'val' (last 1000).\n",
        "            transform (bool): Whether to apply random augmentations.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Split dataset: first 9000 for train, last 1000 for val\n",
        "        if split == 'train':\n",
        "            self.data = self.data.iloc[:9000].reset_index(drop=True)\n",
        "        else:\n",
        "            self.data = self.data.iloc[9000:].reset_index(drop=True)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.shape_names = ['squares', 'circles', 'up', 'right', 'down', 'left']\n",
        "\n",
        "        # Pre-compute class mapping logic\n",
        "        # Generate all unique pairs of indices (i, j) where i < j\n",
        "        self.pairs = []\n",
        "        for i in range(6):\n",
        "            for j in range(i + 1, 6):\n",
        "                self.pairs.append((i, j))\n",
        "\n",
        "    def _get_class_id(self, counts):\n",
        "        \"\"\"\n",
        "        Maps the 6-dimensional count vector to a class ID (0-134).\n",
        "        Logic: Find the two non-zero shapes (i, j).\n",
        "        Class is determined by the pair index and the count of the first shape.\n",
        "        \"\"\"\n",
        "        indices = np.nonzero(counts)[0]\n",
        "        if len(indices) != 2:\n",
        "            # Fallback for unexpected data, though dataset guarantees exactly 2 types\n",
        "            return 0\n",
        "\n",
        "        idx1, idx2 = indices[0], indices[1] # idx1 is always < idx2 because numpy returns sorted\n",
        "\n",
        "        # Find which pair number this is (0 to 14)\n",
        "        pair_idx = self.pairs.index((idx1, idx2))\n",
        "\n",
        "        # Calculate offset based on the count of the first shape (1-9)\n",
        "        # range 1-9 maps to 0-8\n",
        "        count_offset = int(counts[idx1]) - 1\n",
        "\n",
        "        # Total class ID\n",
        "        return pair_idx * 9 + count_offset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('L') # Load as grayscale\n",
        "\n",
        "        # Get counts as numpy array\n",
        "        counts = self.data.iloc[idx, 1:].values.astype(np.float32)\n",
        "\n",
        "        # Apply Augmentations manually to handle label dependencies\n",
        "        if self.transform:\n",
        "            # 1. Random Horizontal Flip\n",
        "            if random.random() > 0.5:\n",
        "                image = transforms.functional.hflip(image)\n",
        "                # Horizontal flip swaps: Right (3) <-> Left (5)\n",
        "                counts[3], counts[5] = counts[5], counts[3]\n",
        "\n",
        "            # 2. Random Vertical Flip\n",
        "            if random.random() > 0.5:\n",
        "                image = transforms.functional.vflip(image)\n",
        "                # Vertical flip swaps: Up (2) <-> Down (4)\n",
        "                counts[2], counts[4] = counts[4], counts[2]\n",
        "\n",
        "            # 3. Random 90 Degree Rotation (Clockwise)\n",
        "            # Options: 0, 90, 180, 270\n",
        "            k = random.randint(0, 3)\n",
        "            if k > 0:\n",
        "                # Rotate image k * 90 degrees clockwise\n",
        "                # TF.rotate expects degrees counter-clockwise. -90 is 90 clockwise.\n",
        "                angle = -90 * k\n",
        "                image = transforms.functional.rotate(image, angle)\n",
        "\n",
        "                # Rotate labels k times: Up(2)->Right(3)->Down(4)->Left(5)->Up(2)\n",
        "                # Indices: 2, 3, 4, 5\n",
        "                sub_counts = [counts[2], counts[3], counts[4], counts[5]]\n",
        "                # Shift array right by k steps\n",
        "                # If k=1 (90 deg): Up(idx 0) moves to Right(idx 1)\n",
        "                shifted = np.roll(sub_counts, k)\n",
        "                counts[2], counts[3], counts[4], counts[5] = shifted\n",
        "\n",
        "            # 4. Random Brightness (Mild)\n",
        "            if random.random() > 0.5:\n",
        "                image = transforms.ColorJitter(brightness=0.2)(image)\n",
        "\n",
        "            # Convert to Tensor\n",
        "            image = transforms.functional.to_tensor(image)\n",
        "\n",
        "            # 5. Gaussian Noise\n",
        "            if random.random() > 0.5:\n",
        "                noise = torch.randn_like(image) * 0.05\n",
        "                image = image + noise\n",
        "                image = torch.clamp(image, 0, 1)\n",
        "\n",
        "            # 6. Random Erasing\n",
        "            if random.random() > 0.5:\n",
        "                # Use pytorch's random erasing\n",
        "                eraser = transforms.RandomErasing(p=1, scale=(0.02, 0.1))\n",
        "                image = eraser(image)\n",
        "\n",
        "        else:\n",
        "            image = transforms.functional.to_tensor(image)\n",
        "\n",
        "        # Get Classification Label (computed AFTER augmentation, though counts define the class)\n",
        "        # Note: Transformations like rotation might change which shape is 'Up',\n",
        "        # potentially changing the class ID if the pair or counts shift.\n",
        "        class_label = self._get_class_id(counts)\n",
        "\n",
        "        return image, torch.tensor(class_label, dtype=torch.long), torch.from_numpy(counts)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = GSNDataset('data/', 'data/labels.csv', split='train', transform=True)\n",
        "val_dataset = GSNDataset('data/', 'data/labels.csv', split='val', transform=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "dm0UXP_AlWBE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultitaskNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultitaskNetwork, self).__init__()\n",
        "\n",
        "        # Fixed Backbone (Feature Extractor) - DO NOT MODIFY\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Head 1: Classification (135 classes)\n",
        "        self.head_cls = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 135) # Output logits (log-probs handled by CrossEntropy/NLL)\n",
        "        )\n",
        "\n",
        "        # Head 2: Regression (6 counts)\n",
        "        self.head_cnt = nn.Sequential(\n",
        "            nn.Linear(256, 6) # Output raw counts\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Classification output (LogSoftmax for NLLLoss)\n",
        "        cls_logits = self.head_cls(features)\n",
        "        cls_log_probs = F.log_softmax(cls_logits, dim=1)\n",
        "\n",
        "        # Regression output\n",
        "        cnt_preds = self.head_cnt(features)\n",
        "\n",
        "        return cls_log_probs, cnt_preds"
      ],
      "metadata": {
        "id": "1-8YYlnPloRZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_experiment(setting_name, lambda_cnt, ignore_cls=False):\n",
        "    \"\"\"\n",
        "    Runs a full training loop for a specific experiment setting.\n",
        "\n",
        "    Args:\n",
        "        setting_name (str): Name of the experiment.\n",
        "        lambda_cnt (float): Weight for the regression loss.\n",
        "        ignore_cls (bool): If True, sets classification loss to 0 (Regression Only).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting Experiment: {setting_name} ---\")\n",
        "\n",
        "    model = MultitaskNetwork().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_cls = nn.NLLLoss()\n",
        "    criterion_cnt = nn.SmoothL1Loss()\n",
        "\n",
        "    # History tracking\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'val_acc': [], 'val_rmse': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    epochs = 100\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels_cls, labels_cnt in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels_cls = labels_cls.to(device)\n",
        "            labels_cnt = labels_cnt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs, counts_pred = model(images)\n",
        "\n",
        "            # Calculate losses based on settings\n",
        "            loss_cls = criterion_cls(log_probs, labels_cls)\n",
        "            loss_cnt = criterion_cnt(counts_pred, labels_cnt)\n",
        "\n",
        "            if ignore_cls:\n",
        "                loss = lambda_cnt * loss_cnt\n",
        "            else:\n",
        "                loss = loss_cls + lambda_cnt * loss_cnt\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds_cnt = []\n",
        "        all_labels_cnt = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels_cls, labels_cnt in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels_cls = labels_cls.to(device)\n",
        "                labels_cnt = labels_cnt.to(device)\n",
        "\n",
        "                log_probs, counts_pred = model(images)\n",
        "\n",
        "                # Validation Loss\n",
        "                l_cls = criterion_cls(log_probs, labels_cls)\n",
        "                l_cnt = criterion_cnt(counts_pred, labels_cnt)\n",
        "\n",
        "                if ignore_cls:\n",
        "                    total_l = lambda_cnt * l_cnt\n",
        "                else:\n",
        "                    total_l = l_cls + lambda_cnt * l_cnt\n",
        "\n",
        "                val_loss += total_l.item()\n",
        "\n",
        "                # Metrics: Accuracy\n",
        "                _, predicted = torch.max(log_probs.data, 1)\n",
        "                total += labels_cls.size(0)\n",
        "                correct += (predicted == labels_cls).sum().item()\n",
        "\n",
        "                # Metrics: Accumulate for RMSE\n",
        "                all_preds_cnt.append(counts_pred.cpu())\n",
        "                all_labels_cnt.append(labels_cnt.cpu())\n",
        "\n",
        "        # Aggregation\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        all_preds_cnt = torch.cat(all_preds_cnt).numpy()\n",
        "        all_labels_cnt = torch.cat(all_labels_cnt).numpy()\n",
        "        val_rmse = np.sqrt(mean_squared_error(all_labels_cnt, all_preds_cnt))\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "        history['val_rmse'].append(val_rmse)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | Val RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model state if needed\n",
        "            torch.save(model.state_dict(), f'best_model_{setting_name}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    return history, model\n",
        "\n",
        "# Helper to evaluate comprehensive metrics for the final report\n",
        "def evaluate_final_metrics(model):\n",
        "    model.eval()\n",
        "    all_preds_cls = []\n",
        "    all_labels_cls = []\n",
        "    all_preds_cnt = []\n",
        "    all_labels_cnt = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels_cls, labels_cnt in val_loader:\n",
        "            images = images.to(device)\n",
        "            log_probs, counts_pred = model(images)\n",
        "\n",
        "            _, predicted = torch.max(log_probs.data, 1)\n",
        "\n",
        "            all_preds_cls.extend(predicted.cpu().numpy())\n",
        "            all_labels_cls.extend(labels_cls.numpy())\n",
        "            all_preds_cnt.append(counts_pred.cpu())\n",
        "            all_labels_cnt.append(labels_cnt.cpu())\n",
        "\n",
        "    all_preds_cnt = torch.cat(all_preds_cnt).numpy()\n",
        "    all_labels_cnt = torch.cat(all_labels_cnt).numpy()\n",
        "\n",
        "    # 1. Classification Metrics\n",
        "    acc = np.mean(np.array(all_preds_cls) == np.array(all_labels_cls))\n",
        "    f1 = f1_score(all_labels_cls, all_preds_cls, average='macro')\n",
        "\n",
        "    # 2. Regression Metrics\n",
        "    mse = mean_squared_error(all_labels_cnt, all_preds_cnt)\n",
        "    rmse_overall = np.sqrt(mse)\n",
        "    mae_overall = mean_absolute_error(all_labels_cnt, all_preds_cnt)\n",
        "\n",
        "    print(f\"\\nFinal Evaluation:\")\n",
        "    print(f\"Top-1 Accuracy: {acc*100:.2f}%\")\n",
        "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "    print(f\"Overall RMSE: {rmse_overall:.4f}\")\n",
        "    print(f\"Overall MAE: {mae_overall:.4f}\")\n",
        "\n",
        "    return acc, rmse_overall"
      ],
      "metadata": {
        "id": "yH5YrXCOluQ7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Classification Only (lambda = 0)\n",
        "hist_1, model_1 = train_experiment(\"Classification_Only\", lambda_cnt=0.0)\n",
        "\n",
        "# Experiment 2: Regression Only (ignore classification loss)\n",
        "# lambda acts as a scaler, we set it to 1.0 for the regression loss weight\n",
        "hist_2, model_2 = train_experiment(\"Regression_Only\", lambda_cnt=1.0, ignore_cls=True)\n",
        "\n",
        "# Experiment 3: Multitask (lambda = 1.0)\n",
        "hist_3, model_3 = train_experiment(\"Multitask\", lambda_cnt=1.0)"
      ],
      "metadata": {
        "id": "Bz4yDVfPl3aw",
        "outputId": "e97b159a-00e3-4250-fff8-218934b3450c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Experiment: Classification_Only ---\n",
            "Epoch [1/100] Loss: 4.7505 | Val Loss: 4.6828 | Val Acc: 0.80% | Val RMSE: 3.1260\n",
            "Epoch [2/100] Loss: 4.6802 | Val Loss: 4.5876 | Val Acc: 2.20% | Val RMSE: 3.1104\n",
            "Epoch [3/100] Loss: 4.1795 | Val Loss: 3.4751 | Val Acc: 8.30% | Val RMSE: 3.1733\n",
            "Epoch [4/100] Loss: 3.4146 | Val Loss: 2.9901 | Val Acc: 11.90% | Val RMSE: 3.3018\n",
            "Epoch [5/100] Loss: 2.9478 | Val Loss: 2.5189 | Val Acc: 17.70% | Val RMSE: 3.4020\n",
            "Epoch [6/100] Loss: 2.6706 | Val Loss: 2.2505 | Val Acc: 23.60% | Val RMSE: 3.4674\n",
            "Epoch [7/100] Loss: 2.4612 | Val Loss: 1.9651 | Val Acc: 27.70% | Val RMSE: 3.5421\n",
            "Epoch [8/100] Loss: 2.2404 | Val Loss: 1.8380 | Val Acc: 32.20% | Val RMSE: 3.5625\n",
            "Epoch [9/100] Loss: 2.1362 | Val Loss: 1.7417 | Val Acc: 33.90% | Val RMSE: 3.5736\n",
            "Epoch [10/100] Loss: 2.0224 | Val Loss: 1.7744 | Val Acc: 30.30% | Val RMSE: 3.6078\n",
            "Epoch [11/100] Loss: 1.9713 | Val Loss: 1.6105 | Val Acc: 34.70% | Val RMSE: 3.6978\n",
            "Epoch [12/100] Loss: 1.8900 | Val Loss: 1.5934 | Val Acc: 35.20% | Val RMSE: 3.7234\n",
            "Epoch [13/100] Loss: 1.8607 | Val Loss: 1.5548 | Val Acc: 35.10% | Val RMSE: 3.6952\n",
            "Epoch [14/100] Loss: 1.8248 | Val Loss: 1.5561 | Val Acc: 35.30% | Val RMSE: 3.6881\n",
            "Epoch [15/100] Loss: 1.7974 | Val Loss: 1.5174 | Val Acc: 38.00% | Val RMSE: 3.6903\n",
            "Epoch [16/100] Loss: 1.7560 | Val Loss: 1.4692 | Val Acc: 38.70% | Val RMSE: 3.6834\n",
            "Epoch [17/100] Loss: 1.7516 | Val Loss: 1.4487 | Val Acc: 38.70% | Val RMSE: 3.7410\n",
            "Epoch [18/100] Loss: 1.7083 | Val Loss: 1.4389 | Val Acc: 40.70% | Val RMSE: 3.7065\n",
            "Epoch [19/100] Loss: 1.6980 | Val Loss: 1.3856 | Val Acc: 40.90% | Val RMSE: 3.7769\n",
            "Epoch [20/100] Loss: 1.6729 | Val Loss: 1.4011 | Val Acc: 40.80% | Val RMSE: 3.6929\n",
            "Epoch [21/100] Loss: 1.6511 | Val Loss: 1.4706 | Val Acc: 37.90% | Val RMSE: 3.7711\n",
            "Epoch [22/100] Loss: 1.6234 | Val Loss: 1.3693 | Val Acc: 42.10% | Val RMSE: 3.7771\n",
            "Epoch [23/100] Loss: 1.6091 | Val Loss: 1.3875 | Val Acc: 40.90% | Val RMSE: 3.8022\n",
            "Epoch [24/100] Loss: 1.5913 | Val Loss: 1.3560 | Val Acc: 42.60% | Val RMSE: 3.7675\n",
            "Epoch [25/100] Loss: 1.5659 | Val Loss: 1.3309 | Val Acc: 44.30% | Val RMSE: 3.7605\n",
            "Epoch [26/100] Loss: 1.5622 | Val Loss: 1.3131 | Val Acc: 42.70% | Val RMSE: 3.7551\n",
            "Epoch [27/100] Loss: 1.5406 | Val Loss: 1.3987 | Val Acc: 41.20% | Val RMSE: 3.7065\n",
            "Epoch [28/100] Loss: 1.5578 | Val Loss: 1.3745 | Val Acc: 41.10% | Val RMSE: 3.6978\n",
            "Epoch [29/100] Loss: 1.5227 | Val Loss: 1.3141 | Val Acc: 42.20% | Val RMSE: 3.7012\n",
            "Epoch [30/100] Loss: 1.5061 | Val Loss: 1.2960 | Val Acc: 42.20% | Val RMSE: 3.7779\n",
            "Epoch [31/100] Loss: 1.5005 | Val Loss: 1.2938 | Val Acc: 43.10% | Val RMSE: 3.7229\n",
            "Epoch [32/100] Loss: 1.4938 | Val Loss: 1.3252 | Val Acc: 42.60% | Val RMSE: 3.7462\n",
            "Epoch [33/100] Loss: 1.4827 | Val Loss: 1.3164 | Val Acc: 42.60% | Val RMSE: 3.7306\n",
            "Epoch [34/100] Loss: 1.4803 | Val Loss: 1.2691 | Val Acc: 44.00% | Val RMSE: 3.6845\n",
            "Epoch [35/100] Loss: 1.4523 | Val Loss: 1.3146 | Val Acc: 43.20% | Val RMSE: 3.7709\n",
            "Epoch [36/100] Loss: 1.4493 | Val Loss: 1.2822 | Val Acc: 43.80% | Val RMSE: 3.7677\n",
            "Epoch [37/100] Loss: 1.4326 | Val Loss: 1.3545 | Val Acc: 41.00% | Val RMSE: 3.7395\n",
            "Epoch [38/100] Loss: 1.4234 | Val Loss: 1.2993 | Val Acc: 43.40% | Val RMSE: 3.7041\n",
            "Epoch [39/100] Loss: 1.4028 | Val Loss: 1.2592 | Val Acc: 46.30% | Val RMSE: 3.7346\n",
            "Epoch [40/100] Loss: 1.4140 | Val Loss: 1.2824 | Val Acc: 45.20% | Val RMSE: 3.7353\n",
            "Epoch [41/100] Loss: 1.3951 | Val Loss: 1.2332 | Val Acc: 45.30% | Val RMSE: 3.7448\n",
            "Epoch [42/100] Loss: 1.3791 | Val Loss: 1.2319 | Val Acc: 45.80% | Val RMSE: 3.7378\n",
            "Epoch [43/100] Loss: 1.3673 | Val Loss: 1.2478 | Val Acc: 45.90% | Val RMSE: 3.7445\n",
            "Epoch [44/100] Loss: 1.3711 | Val Loss: 1.2137 | Val Acc: 46.50% | Val RMSE: 3.6833\n",
            "Epoch [45/100] Loss: 1.3475 | Val Loss: 1.2001 | Val Acc: 46.90% | Val RMSE: 3.7356\n",
            "Epoch [46/100] Loss: 1.3545 | Val Loss: 1.2779 | Val Acc: 42.80% | Val RMSE: 3.6522\n",
            "Epoch [47/100] Loss: 1.3398 | Val Loss: 1.1912 | Val Acc: 46.90% | Val RMSE: 3.7637\n",
            "Epoch [48/100] Loss: 1.3321 | Val Loss: 1.1993 | Val Acc: 47.10% | Val RMSE: 3.7032\n",
            "Epoch [49/100] Loss: 1.2956 | Val Loss: 1.2730 | Val Acc: 44.40% | Val RMSE: 3.6987\n",
            "Epoch [50/100] Loss: 1.3068 | Val Loss: 1.2177 | Val Acc: 46.60% | Val RMSE: 3.6653\n",
            "Epoch [51/100] Loss: 1.2915 | Val Loss: 1.2142 | Val Acc: 47.70% | Val RMSE: 3.7640\n",
            "Epoch [52/100] Loss: 1.2916 | Val Loss: 1.1983 | Val Acc: 46.70% | Val RMSE: 3.7775\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Starting Experiment: Regression_Only ---\n",
            "Epoch [1/100] Loss: 1.3910 | Val Loss: 1.2466 | Val Acc: 0.40% | Val RMSE: 2.3221\n",
            "Epoch [2/100] Loss: 0.9336 | Val Loss: 0.6909 | Val Acc: 0.80% | Val RMSE: 1.5347\n",
            "Epoch [3/100] Loss: 0.4921 | Val Loss: 0.3854 | Val Acc: 0.40% | Val RMSE: 1.0435\n",
            "Epoch [4/100] Loss: 0.3705 | Val Loss: 0.3357 | Val Acc: 0.20% | Val RMSE: 0.9491\n",
            "Epoch [5/100] Loss: 0.3329 | Val Loss: 0.3039 | Val Acc: 0.20% | Val RMSE: 0.8892\n",
            "Epoch [6/100] Loss: 0.3060 | Val Loss: 0.2725 | Val Acc: 0.30% | Val RMSE: 0.8325\n",
            "Epoch [7/100] Loss: 0.2846 | Val Loss: 0.3014 | Val Acc: 0.40% | Val RMSE: 0.8869\n",
            "Epoch [8/100] Loss: 0.2730 | Val Loss: 0.2543 | Val Acc: 0.30% | Val RMSE: 0.7993\n",
            "Epoch [9/100] Loss: 0.2527 | Val Loss: 0.2412 | Val Acc: 0.30% | Val RMSE: 0.7705\n",
            "Epoch [10/100] Loss: 0.2482 | Val Loss: 0.2397 | Val Acc: 0.20% | Val RMSE: 0.7646\n",
            "Epoch [11/100] Loss: 0.2345 | Val Loss: 0.2256 | Val Acc: 0.20% | Val RMSE: 0.7416\n",
            "Epoch [12/100] Loss: 0.2289 | Val Loss: 0.2322 | Val Acc: 0.20% | Val RMSE: 0.7498\n",
            "Epoch [13/100] Loss: 0.2231 | Val Loss: 0.2299 | Val Acc: 0.20% | Val RMSE: 0.7437\n",
            "Epoch [14/100] Loss: 0.2187 | Val Loss: 0.2066 | Val Acc: 0.20% | Val RMSE: 0.7031\n",
            "Epoch [15/100] Loss: 0.2097 | Val Loss: 0.2009 | Val Acc: 0.20% | Val RMSE: 0.6897\n",
            "Epoch [16/100] Loss: 0.2042 | Val Loss: 0.2138 | Val Acc: 0.20% | Val RMSE: 0.7151\n",
            "Epoch [17/100] Loss: 0.2034 | Val Loss: 0.2056 | Val Acc: 0.20% | Val RMSE: 0.6984\n",
            "Epoch [18/100] Loss: 0.1923 | Val Loss: 0.1882 | Val Acc: 0.20% | Val RMSE: 0.6653\n",
            "Epoch [19/100] Loss: 0.1907 | Val Loss: 0.1951 | Val Acc: 0.10% | Val RMSE: 0.6794\n",
            "Epoch [20/100] Loss: 0.1884 | Val Loss: 0.1903 | Val Acc: 0.20% | Val RMSE: 0.6699\n",
            "Epoch [21/100] Loss: 0.1814 | Val Loss: 0.2083 | Val Acc: 0.20% | Val RMSE: 0.7007\n",
            "Epoch [22/100] Loss: 0.1775 | Val Loss: 0.1992 | Val Acc: 0.10% | Val RMSE: 0.6855\n",
            "Epoch [23/100] Loss: 0.1768 | Val Loss: 0.1770 | Val Acc: 0.20% | Val RMSE: 0.6389\n",
            "Epoch [24/100] Loss: 0.1725 | Val Loss: 0.1764 | Val Acc: 0.20% | Val RMSE: 0.6377\n",
            "Epoch [25/100] Loss: 0.1718 | Val Loss: 0.1814 | Val Acc: 0.20% | Val RMSE: 0.6481\n",
            "Epoch [26/100] Loss: 0.1699 | Val Loss: 0.1705 | Val Acc: 0.10% | Val RMSE: 0.6268\n",
            "Epoch [27/100] Loss: 0.1630 | Val Loss: 0.1678 | Val Acc: 0.20% | Val RMSE: 0.6201\n",
            "Epoch [28/100] Loss: 0.1652 | Val Loss: 0.1853 | Val Acc: 0.10% | Val RMSE: 0.6556\n",
            "Epoch [29/100] Loss: 0.1570 | Val Loss: 0.1845 | Val Acc: 0.10% | Val RMSE: 0.6576\n",
            "Epoch [30/100] Loss: 0.1540 | Val Loss: 0.1620 | Val Acc: 0.10% | Val RMSE: 0.6073\n",
            "Epoch [31/100] Loss: 0.1534 | Val Loss: 0.1698 | Val Acc: 0.10% | Val RMSE: 0.6214\n",
            "Epoch [32/100] Loss: 0.1469 | Val Loss: 0.1718 | Val Acc: 0.10% | Val RMSE: 0.6259\n",
            "Epoch [33/100] Loss: 0.1451 | Val Loss: 0.1627 | Val Acc: 0.10% | Val RMSE: 0.6072\n",
            "Epoch [34/100] Loss: 0.1447 | Val Loss: 0.1711 | Val Acc: 0.10% | Val RMSE: 0.6250\n",
            "Epoch [35/100] Loss: 0.1424 | Val Loss: 0.1631 | Val Acc: 0.30% | Val RMSE: 0.6074\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Starting Experiment: Multitask ---\n",
            "Epoch [1/100] Loss: 6.1425 | Val Loss: 5.7235 | Val Acc: 2.70% | Val RMSE: 2.6290\n",
            "Epoch [2/100] Loss: 5.1235 | Val Loss: 3.9593 | Val Acc: 12.50% | Val RMSE: 1.7264\n",
            "Epoch [3/100] Loss: 3.7882 | Val Loss: 3.2021 | Val Acc: 17.30% | Val RMSE: 1.3960\n",
            "Epoch [4/100] Loss: 3.1133 | Val Loss: 2.5672 | Val Acc: 23.70% | Val RMSE: 1.1585\n",
            "Epoch [5/100] Loss: 2.7263 | Val Loss: 2.3435 | Val Acc: 27.70% | Val RMSE: 1.0494\n",
            "Epoch [6/100] Loss: 2.5679 | Val Loss: 2.2116 | Val Acc: 30.10% | Val RMSE: 1.0140\n",
            "Epoch [7/100] Loss: 2.4310 | Val Loss: 2.0364 | Val Acc: 32.10% | Val RMSE: 0.9472\n",
            "Epoch [8/100] Loss: 2.3056 | Val Loss: 2.0313 | Val Acc: 33.50% | Val RMSE: 0.9530\n",
            "Epoch [9/100] Loss: 2.2054 | Val Loss: 1.8854 | Val Acc: 36.60% | Val RMSE: 0.9016\n",
            "Epoch [10/100] Loss: 2.1639 | Val Loss: 1.8769 | Val Acc: 34.50% | Val RMSE: 0.8751\n",
            "Epoch [11/100] Loss: 2.0942 | Val Loss: 1.7696 | Val Acc: 39.60% | Val RMSE: 0.8394\n",
            "Epoch [12/100] Loss: 2.0659 | Val Loss: 1.7762 | Val Acc: 38.30% | Val RMSE: 0.8396\n",
            "Epoch [13/100] Loss: 2.0063 | Val Loss: 1.7570 | Val Acc: 40.50% | Val RMSE: 0.8379\n",
            "Epoch [14/100] Loss: 1.9841 | Val Loss: 1.7041 | Val Acc: 41.50% | Val RMSE: 0.8103\n",
            "Epoch [15/100] Loss: 1.9538 | Val Loss: 1.7305 | Val Acc: 39.20% | Val RMSE: 0.8090\n",
            "Epoch [16/100] Loss: 1.9196 | Val Loss: 1.6706 | Val Acc: 42.80% | Val RMSE: 0.7843\n",
            "Epoch [17/100] Loss: 1.8824 | Val Loss: 1.6705 | Val Acc: 40.00% | Val RMSE: 0.7896\n",
            "Epoch [18/100] Loss: 1.8583 | Val Loss: 1.6809 | Val Acc: 40.50% | Val RMSE: 0.8063\n",
            "Epoch [19/100] Loss: 1.8456 | Val Loss: 1.6805 | Val Acc: 42.10% | Val RMSE: 0.8030\n",
            "Epoch [20/100] Loss: 1.8259 | Val Loss: 1.6043 | Val Acc: 40.30% | Val RMSE: 0.7559\n",
            "Epoch [21/100] Loss: 1.8079 | Val Loss: 1.5594 | Val Acc: 43.80% | Val RMSE: 0.7509\n",
            "Epoch [22/100] Loss: 1.7845 | Val Loss: 1.6229 | Val Acc: 44.10% | Val RMSE: 0.7864\n",
            "Epoch [23/100] Loss: 1.7682 | Val Loss: 1.5482 | Val Acc: 43.50% | Val RMSE: 0.7398\n",
            "Epoch [24/100] Loss: 1.7484 | Val Loss: 1.5791 | Val Acc: 45.00% | Val RMSE: 0.7496\n",
            "Epoch [25/100] Loss: 1.7183 | Val Loss: 1.5205 | Val Acc: 42.70% | Val RMSE: 0.7131\n",
            "Epoch [26/100] Loss: 1.7266 | Val Loss: 1.5507 | Val Acc: 44.50% | Val RMSE: 0.7571\n",
            "Epoch [27/100] Loss: 1.6901 | Val Loss: 1.4968 | Val Acc: 45.40% | Val RMSE: 0.7006\n",
            "Epoch [28/100] Loss: 1.6890 | Val Loss: 1.4851 | Val Acc: 45.10% | Val RMSE: 0.7102\n",
            "Epoch [29/100] Loss: 1.6648 | Val Loss: 1.4961 | Val Acc: 44.80% | Val RMSE: 0.7075\n",
            "Epoch [30/100] Loss: 1.6395 | Val Loss: 1.4844 | Val Acc: 46.40% | Val RMSE: 0.7205\n",
            "Epoch [31/100] Loss: 1.6239 | Val Loss: 1.4829 | Val Acc: 44.50% | Val RMSE: 0.7045\n",
            "Epoch [32/100] Loss: 1.6524 | Val Loss: 1.4647 | Val Acc: 45.50% | Val RMSE: 0.6961\n",
            "Epoch [33/100] Loss: 1.6049 | Val Loss: 1.4697 | Val Acc: 45.20% | Val RMSE: 0.7275\n",
            "Epoch [34/100] Loss: 1.5897 | Val Loss: 1.4383 | Val Acc: 47.70% | Val RMSE: 0.7208\n",
            "Epoch [35/100] Loss: 1.5859 | Val Loss: 1.5229 | Val Acc: 42.90% | Val RMSE: 0.7300\n",
            "Epoch [36/100] Loss: 1.5757 | Val Loss: 1.4206 | Val Acc: 47.00% | Val RMSE: 0.6887\n",
            "Epoch [37/100] Loss: 1.5673 | Val Loss: 1.3993 | Val Acc: 47.90% | Val RMSE: 0.6773\n",
            "Epoch [38/100] Loss: 1.5576 | Val Loss: 1.4047 | Val Acc: 47.70% | Val RMSE: 0.6860\n",
            "Epoch [39/100] Loss: 1.5415 | Val Loss: 1.3721 | Val Acc: 50.20% | Val RMSE: 0.6883\n",
            "Epoch [40/100] Loss: 1.5210 | Val Loss: 1.4352 | Val Acc: 46.90% | Val RMSE: 0.7032\n",
            "Epoch [41/100] Loss: 1.5054 | Val Loss: 1.4332 | Val Acc: 46.60% | Val RMSE: 0.6832\n",
            "Epoch [42/100] Loss: 1.4962 | Val Loss: 1.4161 | Val Acc: 47.20% | Val RMSE: 0.6870\n",
            "Epoch [43/100] Loss: 1.5161 | Val Loss: 1.3998 | Val Acc: 47.20% | Val RMSE: 0.6689\n",
            "Epoch [44/100] Loss: 1.4917 | Val Loss: 1.4058 | Val Acc: 48.70% | Val RMSE: 0.6785\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    }
  ]
}